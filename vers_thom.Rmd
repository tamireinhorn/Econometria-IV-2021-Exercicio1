---
title: "Exercise 1"
output: html_notebook
fig_width: 6 
fig_height: 4 
---

## **1. LOADING PACKAGES**

```{r}

library(dreamerr)
library(tidyverse)
library(HARModel)
library(dplyr)
library(tsibble)
library(feasts)
library(stringr)
library(lubridate)
library(anytime)
library(ggplot2)
library(ggthemes)
library(knitr)
library(kableExtra)
library(DT)
library(htmltools)
library(reshape2) #to be used with tidyverse and gglopt2
library(ggpubr)
library(glmnet)
library(maditr)
library(broom)
library(psych)
library(devtools)
install_github('gogonzo/runner')
library(runner)
library(profvis)
library(foreach)
library(doParallel)
registerDoParallel(numCores)
source('Misc.R')  ##This calls the file with our auxiliary functions, keeping our code clean.

```

## **2. DATA PREPARATION**

The function clean_data will be created to clean the files, and it will be applied in the second chunk of the code, in which we will import the files.

```{r}
clean_data <- function(ticker) {

  ##In this function, we're going to first convert the Date column to date:
 
     ticker$Date <- as.Date(ticker$Date, '%m/%d/%Y')
  
  ## Then, we want to restrict it from 2000 onwards

     new_ticker <- filter(ticker, format(ticker$Date, '%Y') >= 2000)

     ##We need all days to contain the same number ofminutes, so we will use the left_join command first ensure that, and also to create NA for all MINUTES for which certain stocks do not have data.
     
     minute_list <- sort(unique(new_ticker$Time)) 
     day_list <- sort(unique(new_ticker$Date))
     minute_df <- data.frame('Time' = minute_list) ##A minute df is needed for the left_join() to be applied
     day_df <- data.frame('Date' = day_list) ##A day df is needed for the left_join() to be applied
     day_minute_df <- left_join(day_df, minute_df, character()) ##A df is created in which all 390 possible minutes were joined into each possible day
     new_ticker <- left_join(day_minute_df, new_ticker, by = c('Time', 'Date')) ##Then we left join all the informations in the new ticker into the day_minute df, uniformising all dates for all stocks in number of minutes
     new_ticker$minute_return <- (new_ticker$Close - lag(new_ticker$Close))/ new_ticker$Close ##Return taken on levels: p_t - p_{t-1} / p_{t-1}
     new_ticker$minute_log_return <- log(new_ticker$Close) - log(lag(new_ticker$Close)) ##Log return: log(p_t) - log(p_{t-1})
     
  return(new_ticker)
}
```

Given the clean_data() function above, we will now jointly import and clean the data (alternatively, we could have importer everything, then created the cleaning function and then have applied it, using three different chunks). 


```{r}
files_list <- list.files('Data')
stock_name <- str_remove(files_list, ".txt")
stock_list <- vector(mode = "list", length = 29) ##Initialize the stock list.
for (i in 1:length(stock_list)) {
  stock_df <- data.frame(read_csv(paste('Data/',files_list[i], sep = ""), col_types = cols())) 
  clean_stock_df <- clean_data(stock_df)
  stock_list[[i]] <- clean_stock_df[,]
}

names(stock_list) <- stock_name
```

A function to compute daily variables of interest will be created, and in a following code it will be applied to our list of stocks. These variables of interest are the realized volatility, sum of daily minute returns and sum of daily volumes traded per minute:

```{r}

#First for AXP, to understand the logic
#AXP %>% group_by(Date) %>% summarise(daily=sum(minute_log_return,na.rm=T), RV = sum(minute_log_return^2, na.rm = T), VOL = sum(Volume, na.rm = T), log_RV = log(sum(minute_log_return^2, na.rm = T)) ) ##This works as intended, just creates our daily dataframe. Therefore

##This logic will be turned into a function that creates daily dataframes for all of the tickers

make_daily_data <- function(clean_ticker){
  daily_ticker <- clean_ticker %>% group_by(Date) %>% summarise(daily=sum(minute_log_return,na.rm=T), RV = sum(minute_log_return^2, na.rm = T), VOL = sum(Volume, na.rm = T), log_RV = log(sum(minute_log_return^2, na.rm = T)) )
  #return(daily_ticker)
}
```

Now, we apply this function for all tickers, just like before.

```{r}
daily_stock_list <- vector(mode = 'list', length = 29) ##Initialize the daily stock list.

for (i in 1:length(stock_list)) {

  daily_stock_df <- make_daily_data(stock_list[[i]])
  daily_stock_list[[i]] <- daily_stock_df[,]
  
}
names(daily_stock_list) <- stock_name
```

As suggested, we should try creating one giant dataframe in order to use facet_wrap and plot everything at once.

```{r}
complete_daily_df <- bind_rows(daily_stock_list, .id = "column_label")
##I just want the name to be Stock:
names(complete_daily_df)[names(complete_daily_df)== 'column_label'] <- 'Stock'
```

## **3. PLOTS**

**Histogram of RV:**

```{r fig.height = 30, fig.width = 14}
ggplot(complete_daily_df, aes(x = RV)) +
  geom_histogram(aes(y = ..density..),binwidth = 0.0001, color = "darkblue", fill = "lightblue") + 
  labs(title = "RV Histograms", x = "", y = "")   + xlim(0, 0.01) +  facet_wrap(~ Stock, ncol = 3) +
  theme_minimal()
```

**Histogram of log RV:**

```{r fig.height = 30, fig.width = 14}

ggplot(complete_daily_df, aes(x = log_RV)) +
  geom_histogram(aes(y = ..density..),binwidth = 0.0001, color = "darkblue", fill = "lightblue") + 
  labs(title = "log(RV) Histograms", x = "", y = "")   + xlim(0, 0.01) +  facet_wrap(~ Stock, ncol = 3) +
  theme_minimal()

```

**Time Series of RV**

```{r fig.height = 30, fig.width = 14}

#test <- complete_daily_df %>%
#        filter(Stock=="AXP")
#ggplot(test,aes(x=Date, y=100*RV))+geom_line

  plot <- ggplot(complete_daily_df, aes(x = Date, y= 100*RV)) +
  geom_line(size=1) +
  facet_wrap(~ Stock, ncol = 4, scales = 'free') +
  labs(x = "", y = "", title = 'RV Time Series (in percentage terms, %)') +
  theme_bw() + theme(legend.position = "none")

  plot
 

```

**Time Series of log_RV**

```{r fig.height = 30, fig.width = 14}

  plot_log_RV <- ggplot(complete_daily_df, aes(x = Date, y= 100*log_RV)) +
  geom_line(size=1) +
  facet_wrap(~ Stock, ncol = 4, scales = 'free') +
  labs(x = "", y = "", title = 'log(RV) Time Series (in percentage terms, %)') +
  theme_bw() + theme(legend.position = "none")

  plot_log_RV
```

**AUTOCORRELATION** :

We will use functions to obtain ACFs for both our data and for simulated AR processes using estimated coefficients of our data.

**ACF for RV and log(RV)**:

```{r fig.height = 40, fig.width = 14}

RV_tsibble <- complete_daily_df %>% select(Stock, Date, RV) %>%  as_tsibble(key = 'Stock', index = 'Date', regular = FALSE)

RV_tsibble %>% ACF(RV, lag_max = 200) %>% autoplot() + theme_bw() + labs(x = 'Lags', y = '')

#estimating an AR(1) process for RV of stock AXP:
#RV_AXP <- RV_tsibble %>% filter(Stock == "AXP")
#RV_AXP_AR1 <- arima(RV_AXP$RV , order = c(1, 0, 0))

#Via TidyVerse, we already generate the ACFs for both level and log RV

autocorr_f <- function(df){
  
  autocorr <- data.frame(matrix(data = seq(0,29,1), nrow = 30, ncol = 1))
  names(autocorr) <- c("Lag")   #seq(0,29,1) indicate that there are 30 lags of estimated ACFs, for each Stock. The above function generates, thus, one dataframe listing ACFs, for each Stock.
  
  autocorr$autocorr_level = acf(df$RV, lag.max = 29, pl = F)[[1]] %>% as.numeric() 
  autocorr$autocorr_log = acf(df$log_RV, lag.max = 29, pl = F)[[1]] %>% as.numeric()     #[[1]] selects the the values of the ACFs themselves, as the function acf() generates other objects that are not of interest
  
  #ACFs for AR Model
  
  autocorr$autocorr_ar_level = ARMAacf(ar = arima(df$RV, order = c(1, 0, 0))$coef[1], ma = 0, lag.max = 29) %>% as.numeric()   
  
#The ARMAacf simulates a model for coefficients we give as imput, and returns the ACFs. @coef[1] selects only the auto-regressive parameter, and not the intercept.
                                     
  autocorr$autocorr_ar_log = ARMAacf(ar = arima(df$log_RV, order = c(1, 0, 0))$coef[1], ma = 0, lag.max = 29) %>% as.numeric()
  
  return(autocorr)
     
}

#the argument autocorr that is taken by the function created above is here defined as being complete_daily_df, which we group_by() and then apply the function

autocorr <- complete_daily_df %>% 
  group_by(Stock) %>% 
  group_modify(~ autocorr_f(.x))


#Now let`s use tidyverse to plot the graphs of interest (for both RV and log RF, it would be interesting to plot bars of the observed ACFs alongside the theoretical ACFs of forecasted ARs)

```

Looking at the ACF for both RV and log(RV), it is easy to notice that the plots present an approximate linear decay, whereas theoretical autoregressive processes contain ACFs that decay exponentially. 

**3. BENCHMARK MODEL**

We will now conduct an estimation of a HAR Model for the realized volatility for each Stock in our sample. With the estimated coefficients, we will simulate a HAR process in order to obtain a theoretical ACF and compare it with the empirical one. Before doing these procedures, we can already note that, from the empirical ACFs calculated over our data, the realised volatility presents a strong persistence over time, and this can be expressed by the rather linear decay observed in the ACF functions. Corsi (2009) notes that the purpose of reproducing the
long memory of empirical volatility seems be fully achieved by estimating HAR processes for actual data. If we focus on the period of time spanning our sample, approximately 17 years, allowing for less lags, there is a relatively linear decay in the figures of Corsi (2009) that indicate that the HAR model can be effective in reproducing actual volatility.


Trying to estimate da HAR models in a similar way to the AR process in the descriptive analysis

```{r}
autocorr_har_f <- function(df){
  
  autocorr_har <- data.frame(matrix(data = seq(0,29,1), nrow = 30, ncol = 1))
  names(autocorr) <- c("Lag")   #same as for the AR case. For df, we generate 30 rows, where ultimately the ACFs for each lag will be stored as we apply autocorr_har_f() over a df.
  
  autocorr_har$autocorr_level = acf(df$RV, lag.max = 29, pl = F)[[1]] %>% as.numeric() 
  autocorr_har$autocorr_log = acf(df$log_RV, lag.max = 29, pl = F)[[1]] %>% as.numeric()     #same as for the AR case
  
#Estimating the HAR Models
  
har_est_rv <- HAREstimate(RM = df$RV, periods = c(1,5,22))

har_est_log_rv <- HAREstimate(RM = df$log_RV, periods = c(1,5,22))

#Extracting the estimated coefficients and simulating theoretical processes with those coefficients:

har_sim_rv <- HARSimulate(len=1500, periods = c(1, 5, 22), coef=coef(har_est_rv), errorTermSD = 0.001) 

har_sim_log_rv <- HARSimulate(len=1500, periods = c(1, 5, 22), coef=coef(har_est_log_rv), errorTermSD = 0.001) 

#ACFs for simulated HAR Models
  
autocorr_har$autocorr_har_level = acf(har_sim_rv, lag.max = 29) %>% as.numeric()   

autocorr_har$autocorr_har_log = acf(har_sim_log_rv, lag.max = 29) %>% as.numeric()
  
  return(autocorr_har)
}

#the argument autocorr that is taken by the function created above is here defined as being complete_daily_df, our working dataframe, which we group_by() and then apply the function

autocorr_har <- complete_daily_df %>% 
  group_by(Stock) %>% 
  group_modify(~ autocorr_har_f(.x))
```


**Rolling Window Estimations**

Below, we will first conduct a rolling window estimation for RV of company AXP and try to generalize the method to the other companies via tidyverse and group_map() or group_modify()

```{r}

```
Vamos prum código mais limpo pelo amor de deus:
```{r}
full_data <- VariableCreation(df = complete_daily_df)
riiidge <- RidgeRegressionBIC(x, y)
```

