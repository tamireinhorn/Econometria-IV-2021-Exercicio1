---
title: "Exercise 1"
output: html_notebook
fig_width: 6 
fig_height: 4 
---

## **1. LOADING PACKAGES**

```{r}

library(dreamerr)
library(tidyverse)
library(HARModel)
library(dplyr)
library(tsibble)
library(feasts)
library(stringr)
library(lubridate)
library(anytime)
library(ggplot2)
library(ggthemes)
library(knitr)
library(kableExtra)
library(DT)
library(htmltools)
library(reshape2) #to be used with tidyverse and gglopt2
library(ggpubr)
library(glmnet)
library(maditr)
library(broom)
library(devtools)
library(foreach)
library(forecast)
source('Misc.R')  ##This calls the file with our auxiliary functions, keeping our code clean.
install_github('gabrielrvsc/HDeconometrics')
library(HDeconometrics)
library(ipred)
library(boot)
library(tseries)
```

## **2. DATA PREPARATION**

The function clean_data will be created to clean the files, and it will be applied in the second chunk of the code, in which we will import the files.

```{r}
clean_data <- function(ticker) {

  ##In this function, we're going to first convert the Date column to date:
 
     ticker$Date <- as.Date(ticker$Date, '%m/%d/%Y')
  
  ## Then, we want to restrict it from 2000 onwards

     new_ticker <- filter(ticker, format(ticker$Date, '%Y') >= 2000)

     ##We need all days to contain the same number ofminutes, so we will use the left_join command first ensure that, and also to create NA for all MINUTES for which certain stocks do not have data.
     
     minute_list <- sort(unique(new_ticker$Time)) 
     day_list <- sort(unique(new_ticker$Date))
     minute_df <- data.frame('Time' = minute_list) ##A minute df is needed for the left_join() to be applied
     day_df <- data.frame('Date' = day_list) ##A day df is needed for the left_join() to be applied
     day_minute_df <- left_join(day_df, minute_df, character()) ##A df is created in which all 390 possible minutes were joined into each possible day
     new_ticker <- left_join(day_minute_df, new_ticker, by = c('Time', 'Date')) ##Then we left join all the informations in the new ticker into the day_minute df, uniformising all dates for all stocks in number of minutes
     new_ticker$minute_return <- (new_ticker$Close - lag(new_ticker$Close))/ new_ticker$Close ##Return taken on levels: p_t - p_{t-1} / p_{t-1}
     new_ticker$minute_log_return <- log(new_ticker$Close) - log(lag(new_ticker$Close)) ##Log return: log(p_t) - log(p_{t-1})
     
  return(new_ticker)
}
```

Given the clean_data() function above, we will now jointly import and clean the data (alternatively, we could have importer everything, then created the cleaning function and then have applied it, using three different chunks). 


```{r}
files_list <- list.files('Data')
stock_name <- str_remove(files_list, ".txt")
stock_list <- vector(mode = "list", length = 29) ##Initialize the stock list.
for (i in 1:length(stock_list)) {
  stock_df <- data.frame(read_csv(paste('Data/',files_list[i], sep = ""), col_types = cols())) 
  clean_stock_df <- clean_data(stock_df)
  stock_list[[i]] <- clean_stock_df[,]
}

names(stock_list) <- stock_name
```

A function to compute daily variables of interest will be created, and in a following code it will be applied to our list of stocks. These variables of interest are the realized volatility, sum of daily minute returns and sum of daily volumes traded per minute:

```{r}

#First for AXP, to understand the logic
#AXP %>% group_by(Date) %>% summarise(daily=sum(minute_log_return,na.rm=T), RV = sum(minute_log_return^2, na.rm = T), VOL = sum(Volume, na.rm = T), log_RV = log(sum(minute_log_return^2, na.rm = T)) ) ##This works as intended, just creates our daily dataframe. Therefore

##This logic will be turned into a function that creates daily dataframes for all of the tickers

make_daily_data <- function(clean_ticker){
  daily_ticker <- clean_ticker %>% group_by(Date) %>% summarise(daily=sum(minute_log_return,na.rm=T), RV = sum(minute_log_return^2, na.rm = T), VOL = sum(Volume, na.rm = T), log_RV = log(sum(minute_log_return^2, na.rm = T)) )
  #return(daily_ticker)
}
```

Now, we apply this function for all tickers, just like before.

```{r}
daily_stock_list <- vector(mode = 'list', length = 29) ##Initialize the daily stock list.

for (i in 1:length(stock_list)) {

  daily_stock_df <- make_daily_data(stock_list[[i]])
  daily_stock_list[[i]] <- daily_stock_df[,]
  
}
names(daily_stock_list) <- stock_name
```

As suggested, we should try creating one giant dataframe in order to use facet_wrap and plot everything at once.

```{r}
complete_daily_df <- bind_rows(daily_stock_list, .id = "column_label")
##I just want the name to be Stock:
names(complete_daily_df)[names(complete_daily_df)== 'column_label'] <- 'Stock'
```



## **3. PLOTS**

**Histogram of RV:**

```{r fig.height = 30, fig.width = 14}
ggplot(complete_daily_df, aes(x = RV)) +
  geom_histogram(aes(y = ..density..), color = "darkblue", fill = "lightblue") + 
  labs(title = "RV Histograms", x = "", y = "")   +  facet_wrap(~ Stock, ncol = 3, scales = 'free') +
  theme_minimal()
```


Time Series of SPY

```{r}  
time_series_spy <- complete_daily_df %>%  filter(Stock=="SPY")  

plot <- ggplot(time_series_spy, aes(x = Date, y= 100*RV)) +
  geom_line(size=1) +
  labs(x = "", y = "", title = 'RV Time Series (in percentage terms, %)') +
  theme_bw() + theme(legend.position = "none")

  plot

```

**Histogram of log RV:**

```{r fig.height = 30, fig.width = 14}

ggplot((complete_daily_df), aes(x = log_RV)) +
  geom_histogram(color = "darkblue", fill = "lightblue") + 
  labs(title = "log(RV) Histograms", x = "", y = "")   +  facet_wrap(~ Stock, ncol = 3, scales="free") + theme_minimal()

```

**Time Series of RV**

```{r fig.height = 30, fig.width = 14}

#test <- complete_daily_df %>%
#        filter(Stock=="AXP")
#ggplot(test,aes(x=Date, y=100*RV))+geom_line

  plot <- ggplot(complete_daily_df, aes(x = Date, y= 100*RV)) +
  geom_line(size=1) +
  facet_wrap(~ Stock, ncol = 4, scales = 'free') +
  labs(x = "", y = "", title = 'RV Time Series (in percentage terms, %)') +
  theme_bw() + theme(legend.position = "none")

  plot
 

```

**Time Series of log_RV**

```{r fig.height = 30, fig.width = 14}

  plot_log_RV <- ggplot(complete_daily_df, aes(x = Date, y= 100*log_RV)) +
  geom_line(size=1) +
  facet_wrap(~ Stock, ncol = 4, scales = 'free') +
  labs(x = "", y = "", title = 'log(RV) Time Series (in percentage terms, %)') +
  theme_bw() + theme(legend.position = "none")

  plot_log_RV
```

**AUTOCORRELATION** :

We will use functions to obtain ACFs for both our data and for simulated AR processes using estimated coefficients of our data.

**ACF for RV and log(RV)**:

```{r fig.height = 40, fig.width = 14}

autocorr_f <- function(df){
  
  autocorr <- data.frame(matrix(data = seq(0,21,1), nrow = 22, ncol = 1))
  names(autocorr) <- c("Lag")   #seq(0,21,1) indicate that there are 21 lags of estimated ACFs, for each Stock. The above function generates, thus, one dataframe listing ACFs, for each Stock.
  
  autocorr$autocorr_level = acf(df$RV, lag.max = 21, pl = F)[[1]] %>% as.numeric() 
  autocorr$autocorr_log = acf(df$log_RV, lag.max = 21, pl = F)[[1]] %>% as.numeric()     #[[1]] selects the the values of the ACFs themselves, as the function acf() generates other objects that are not of interest
  
  #ACFs for AR Model
  
  autocorr$autocorr_ar_level = ARMAacf(ar = arima(df$RV, order = c(1, 0, 0))$coef[1], ma = 0, lag.max = 21) %>% as.numeric()   
  
#The ARMAacf simulates a model for coefficients we give as imput, and returns the ACFs. @coef[1] selects only the auto-regressive parameter, and not the intercept.
                                     
  autocorr$autocorr_ar_log = ARMAacf(ar = arima(df$log_RV, order = c(1, 0, 0))$coef[1], ma = 0, lag.max = 21) %>% as.numeric()
  
  return(autocorr)
     
}

#the argument autocorr that is taken by the function created above is here defined as being complete_daily_df, which we group_by() and then apply the function

autocorr <- complete_daily_df %>% 
  group_by(Stock) %>% 
  group_modify(~ autocorr_f(.x))

#write.csv(autocorr, "autocorr.csv",row.names = FALSE)
```

Plotting the ACF for our data in Levels and for the implied AR model in levels

```{r}
autocorr %>%
  pivot_longer(-(Stock:Lag), names_to = "type",values_to ="data") %>%
  #above we insert all variables which are not Stock and Lag into a unit column, called   "type", and the corresponding values in a column called "data"
  
  filter(type %in% c("autocorr_ar_level", "autocorr_level")) %>%
  # "%in% is due to the fact that we are working with a vector" 
  ggplot(aes(x = Lag, y = data, fill = type)) +
    geom_col(position="dodge")+
    #dodge puts columns side by side
    facet_wrap(~ Stock) +
    theme_bw() +
    theme(legend.position="top") +
    scale_fill_discrete(labels = c("Autocorrelation for RV in AR model", 
                                   "Autocorrelation for RV in the data")) +
    theme(legend.title = element_blank())
```

Plotting the ACF for our data in log and for the implied AR model in log

```{r}

autocorr %>%
  pivot_longer(-(Stock:Lag), names_to = "type",values_to ="data") %>%
  filter(type %in% c("autocorr_ar_log", "autocorr_log")) %>%
  ggplot(aes(x = Lag, y = data, fill = type)) +
    geom_col(position="dodge")+
    facet_wrap(~ Stock) +
    theme_bw() +
    theme(legend.position="top") +
    scale_fill_discrete(labels = c("Autocorrelation for Log(RV) in AR model", 
                                   "Autocorrelation for log(RV) in the data")) +
    theme(legend.title = element_blank())


  wide_complete_daily_df <- complete_daily_df %>% select(Stock, Date, RV) %>% dcast(Date ~ Stock, value.var = "RV")  #This other_stock df needs to be wide!
  
  wide_complete_daily_df$Date[1001]
  
  dim(wide_complete_daily_df)


```

Looking at the ACF for both RV and log(RV), it is easy to notice that the plots present an approximate linear decay, whereas theoretical autoregressive processes contain ACFs that decay exponentially. 

**ACF for RV and log(RV), now with 100 lags**:

```{r fig.height = 40, fig.width = 14}

autocorr_f <- function(df){
  
  autocorr <- data.frame(matrix(data = seq(0,99,1), nrow = 100, ncol = 1))
  names(autocorr) <- c("Lag") 
  
  autocorr$autocorr_level = acf(df$RV, lag.max = 99, pl = F)[[1]] %>% as.numeric() 
  autocorr$autocorr_log = acf(df$log_RV, lag.max = 99, pl = F)[[1]] %>% as.numeric()  
  #ACFs for AR Model
  
  autocorr$autocorr_ar_level = ARMAacf(ar = arima(df$RV, order = c(1, 0, 0))$coef[1], ma = 0, lag.max = 99) %>% as.numeric()   
  
#The ARMAacf simulates a model for coefficients we give as imput, and returns the ACFs. @coef[1] selects only the auto-regressive parameter, and not the intercept.
                                     
  autocorr$autocorr_ar_log = ARMAacf(ar = arima(df$log_RV, order = c(1, 0, 0))$coef[1], ma = 0, lag.max = 99) %>% as.numeric()
  
  return(autocorr)
     
}

#the argument autocorr that is taken by the function created above is here defined as being complete_daily_df, which we group_by() and then apply the function

autocorr <- complete_daily_df %>% 
  group_by(Stock) %>% 
  group_modify(~ autocorr_f(.x))

write.csv(autocorr, "autocorr.csv",row.names = FALSE)
```

Plotting the ACF for our data in Levels and for the implied AR model in levels

```{r}
autocorr %>%
  pivot_longer(-(Stock:Lag), names_to = "type",values_to ="data") %>%
  #above we insert all variables which are not Stock and Lag into a unit column, called   "type", and the corresponding values in a column called "data"
  
  filter(type %in% c("autocorr_ar_level", "autocorr_level")) %>%
  # "%in% is due to the fact that we are working with a vector" 
  ggplot(aes(x = Lag, y = data, fill = type)) +
    geom_col(position="dodge")+
    #dodge puts columns side by side
    facet_wrap(~ Stock) +
    theme_bw() +
    theme(legend.position="top") +
    scale_fill_discrete(labels = c("Autocorrelation for RV in AR model", 
                                   "Autocorrelation for RV in the data")) +
    theme(legend.title = element_blank())
```


**3. BENCHMARK MODEL**

We will now conduct an estimation of a HAR Model for the realized volatility for each Stock in our sample. With the estimated coefficients, we will simulate a HAR process in order to obtain a theoretical ACF and compare it with the empirical one. Before doing these procedures, we can already note that, from the empirical ACFs calculated over our data, the realised volatility presents a strong persistence over time, and this can be expressed by the rather linear decay observed in the ACF functions. Corsi (2009) notes that the purpose of reproducing the
long memory of empirical volatility seems be fully achieved by estimating HAR processes for actual data. If we focus on the period of time spanning our sample, approximately 17 years, allowing for less lags, there is a relatively linear decay in the figures of Corsi (2009) that indicate that the HAR model can be effective in reproducing actual volatility.


Trying to estimate da HAR models in a similar way to the AR process in the descriptive analysis

```{r}
autocorr_har_f <- function(df){
  
  autocorr_har <- data.frame(matrix(data = seq(0,29,1), nrow = 30, ncol = 1))
  names(autocorr) <- c("Lag")   #same as for the AR case. For df, we generate 30 rows, where ultimately the ACFs for each lag will be stored as we apply autocorr_har_f() over a df.
  
  autocorr_har$autocorr_level = acf(df$RV, lag.max = 29, pl = F)[[1]] 
  
  autocorr_har$autocorr_log = acf(df$log_RV, lag.max = 29, pl = F)[[1]] 
  
#Estimating the HAR Models
  
har_est_rv <- HAREstimate(RM = df$RV, periods = c(1,5,22))

har_est_log_rv <- HAREstimate(RM = df$log_RV, periods = c(1,5,22))

#Extracting the estimated coefficients and simulating theoretical processes with those coefficients:

#I think for each Stock we have 4310 daily observations (perhaps only V does not have this number, so this is the proper len() in the function below)

har_sim_rv <- HARSimulate(len=4310, periods = c(1, 5, 22), coef=coef(har_est_rv), errorTermSD = 0.001) 

har_sim_log_rv <- HARSimulate(len=4310, periods = c(1, 5, 22), coef=coef(har_est_log_rv), errorTermSD = 0.001) 

#ACFs for simulated HAR Models
  
autocorr_har$autocorr_har_level = acf(har_sim_rv@simulation, lag.max = 29)    

autocorr_har$autocorr_har_log = acf(har_sim_log_rv@simulation, lag.max = 29) 
  
return(autocorr_har)
}

#the argument autocorr that is taken by the function created above is here defined as being complete_daily_df, our working dataframe, which we group_by() and then apply the function

autocorr_har <- complete_daily_df %>% 
  group_by(Stock) %>% 
  group_modify(~ autocorr_har_f(.x))

#write.csv(autocorr_har, "autocorr_har.csv",row.names = FALSE)

for(i in seq(1, nrow(autocorr_har),by=30)) {
  
  c <- autocorr_har$autocorr_har_level[[i]]
  d <- autocorr_har$autocorr_har_log[[i]]
  
    for(j in 0:29){
    autocorr_har$autocorr_har_level[[i+j]] <- c[1+j]
    autocorr_har$autocorr_har_log[[i+j]] <- d[1+j]
    }
}

colnames(autocorr_har)[2] <- "Lag"


#Because the HAR values are in the category "list", while our data is in the category "numeric", we need to convert the HAR data into numeric to plot our graphs

autocorr_har$autocorr_har_level <- as.numeric(unlist(autocorr_har$autocorr_har_level))
autocorr_har$autocorr_har_log <- as.numeric(unlist(autocorr_har$autocorr_har_log))

```

Plotting the ACF for our data in Levels and for the implied HAR model in levels

```{r}
autocorr_har %>%
  pivot_longer(-(Stock:Lag), names_to = "type",values_to ="data") %>%
  filter(type %in% c("autocorr_har_level", "autocorr_level")) %>%
  ggplot(aes(x = Lag, y = data, fill = type)) +
    geom_col(position="dodge")+
    #dodge puts columns side by side
    facet_wrap(~ Stock) +
    theme_bw() +
    theme(legend.position="top") +
    scale_fill_discrete(labels = c("Autocorrelation HAR Level", 
                                   "Autocorrelation Level")) +
    theme(legend.title = element_blank())
```
Plotting the ACF for our data in LOG and for the implied HAR model in LOG

```{r}
autocorr_har %>%
  pivot_longer(-(Stock:Lag), names_to = "type",values_to ="data") %>%
  filter(type %in% c("autocorr_har_log", "autocorr_log")) %>%
  ggplot(aes(x = Lag, y = data, fill = type)) +
    geom_col(position="dodge")+
    #dodge puts columns side by side
    facet_wrap(~ Stock) +
    theme_bw() +
    theme(legend.position="top") +
    scale_fill_discrete(labels = c("Autocorrelation HAR log", 
                                   "Autocorrelation log")) +
    theme(legend.title = element_blank())
```

**Rolling Window Estimations**
```{r}
full_data <- VariableCreation(df = complete_daily_df) ##We use this function to create the desired variables.
complete_ML_set <- RollingWindow(full_data)

```
Now, we can extract the objects and calculate MSEs and do the Diebold-Mariano test.
```{r}
betas_lasso <-complete_ML_set$betas_lasso 
betas_ridge <- complete_ML_set$betas_ridge
betas_adalasso <- complete_ML_set$betas_adalasso
betas_elastic_net <- complete_ML_set$betas_elastic_net
betas_ada_elastic_net <- complete_ML_set$betas_ada_elastic_net
betas_bagging <- complete_ML_set$betas_bagging
MSE_har <- complete_ML_set$MSE_HAR
MSE_ridge <- complete_ML_set$MSE_ridge
MSE_lasso <- complete_ML_set$MSE_lasso
MSE_adalasso <- complete_ML_set$MSE_adalasso
MSE_elastic_net <- complete_ML_set$MSE_elastic_net
MSE_ada_elastic_net <- complete_ML_set$MSE_ada_elastic_net
MSE_bagging <- complete_ML_set$MSE_bagging
cat('The MSE for the benchmark model is' , mean(MSE_har), '\n')
cat('The MSE for the ridge model relative to the benchmark is' , mean(MSE_ridge)/mean(MSE_har),'and its absolute value is',mean(MSE_ridge), '\n')
cat('The MSE for the LASSO model relative to the benchmark is' , mean(MSE_lasso)/mean(MSE_har),'and its absolute value is',mean(MSE_lasso),'\n')
cat('The MSE for the Adaptative Lasso model relative to the benchmark is' , mean(MSE_adalasso)/mean(MSE_har),'and its absolute value is',mean(MSE_adalasso), '\n')
cat('The MSE for the Elastic Net model relative to the benchmark is' , mean(MSE_elastic_net)/mean(MSE_har),'and its absolute value is',mean(MSE_elastic_net), '\n')
cat('The MSE for the Adaptative Elastic Net model relative to the benchmark is' , mean(MSE_ada_elastic_net)/mean(MSE_har), 'and its absolute value is',mean(MSE_ada_elastic_net),'\n')
cat('The MSE for the bagging model relative to the benchmark is' , mean(MSE_bagging)/mean(MSE_har), 'and its absolute value is',mean(MSE_bagging),'\n')
```
Now for the Diebold-Mariano test:
```{r}
dm.test(MSE_har, MSE_ridge)
dm.test(MSE_har, MSE_lasso)
dm.test(MSE_har, MSE_adalasso)
dm.test(MSE_har, MSE_elastic_net)
dm.test(MSE_har, MSE_ada_elastic_net)
dm.test(MSE_har, MSE_bagging)
```
##TODO Write the code to join these MSEs all into a table with the dates (?)


Regressor frequency test:
```{r}
##First, we create dataframes for every coefficent matrix, transposing them for ease of use
indep_names <- stock_name %>% setdiff(., "SPY") ##Lists all stocks but SPY
var_name <- c('month_RV', 'week_RV', 'previous_RV', indep_names)
betas_ridge_df <- as.data.frame(t(betas_ridge))
names(betas_ridge_df) <- c(var_name)
betas_ridge_df[betas_ridge_df != 0] <- 1 
betas_lasso_df <- as.data.frame(t(betas_lasso))
names(betas_lasso_df) <- var_name
betas_lasso_df[betas_lasso_df != 0] <- 1 
betas_adalasso_df <- as.data.frame(t(betas_adalasso))
names(betas_adalasso_df) <- var_name
betas_adalasso_df[betas_adalasso_df != 0] <- 1 
betas_elastic_net_df <- as.data.frame(t(betas_elastic_net))
names(betas_elastic_net_df) <- var_name
betas_elastic_net_df[betas_elastic_net_df != 0] <- 1 
betas_ada_elastic_net_df <- as.data.frame(t(betas_ada_elastic_net))
names(betas_ada_elastic_net_df) <- var_name
betas_ada_elastic_net_df[betas_ada_elastic_net_df != 0] <- 1 

##Now aggregating all models
betas_aggr <- 1/5 * (betas_ridge_df + betas_lasso_df + betas_adalasso_df + betas_elastic_net_df + betas_ada_elastic_net_df)
##I want to have a list of all the datas where we applied the rolling window.
window_size <- 1000
month <- 22
days <- dim(full_data)[[1]]
#num_windows <- days - (month - 1)  - window_size  Old
num_windows <- days  - window_size +1
##The datas go from the first date in full_data to num_windows.
applied_data_list <- full_data$Date[1:num_windows]
betas_aggr$Date <- applied_data_list

```
Now, to plot the graph of regressor frequency.
```{r fig.height = 23, fig.width = 10}
aggr_plot <- melt(betas_aggr ,  id.vars = 'Date', variable.name = 'series')
ggplot(aggr_plot, aes(Date,value)) + geom_line() + facet_wrap(series ~ ., ncol = 2)
```
Something that should be interesting is to plot the number of non zero coefficients per date for all methods that perform variable selection.

```{r}
lasso_var_num <- as.data.frame(rowSums(betas_lasso_df))
adalasso_var_num <- as.data.frame(rowSums(betas_adalasso_df))
elastic_net_var_num <- as.data.frame(rowSums(betas_elastic_net_df))
ada_elastic_net_var_num <-  as.data.frame(rowSums(betas_elastic_net_df))
lasso_var_num$Date <- applied_data_list

names(lasso_var_num) <- c('VarNum', 'Date')
lasso_var_num$Model <- 'LASSO'
adalasso_var_num$Date <- applied_data_list
names(adalasso_var_num) <- c('VarNum', 'Date')
adalasso_var_num$Model <- 'ADALASSO'
elastic_net_var_num$Date <- applied_data_list
names(elastic_net_var_num) <- c('VarNum', 'Date')
elastic_net_var_num$Model <- 'Elastic Net'
ada_elastic_net_var_num$Date <- applied_data_list
names(ada_elastic_net_var_num) <- c('VarNum', 'Date')
ada_elastic_net_var_num$Model <- 'Adaptative Elastic Net'
total_freq <- bind_rows(lasso_var_num, adalasso_var_num, elastic_net_var_num, ada_elastic_net_var_num)


ggplot(total_freq, aes(Date, VarNum)) + geom_line(aes(colour = Model)) + ggtitle('Chosen Variables in All Models') + xlab('Date') + ylab('Variable Number') +scale_color_manual(values = c("red", "black", "blue", "green"))

##The reason we only see 3 models is that Adaptative Elastic Net has same number of regressors as Elastic Net
##This can be seen via an inner join of the two dfs via VarNum and Date, returning the same number of rows,
##So every date has the same value for both models!

```
Now, for the rolling MSE.
```{r}
MSE_lasso_df <- as.data.frame(MSE_lasso)
names(MSE_lasso_df) <- c('MSE')
MSE_lasso_df$year <-  format(applied_data_list, '%Y')
MSE_lasso_df$model <- 'LASSO'

MSE_ridge_df <- as.data.frame(MSE_ridge)
names(MSE_ridge_df) <- c('MSE')
MSE_ridge_df$year <- format(applied_data_list, '%Y')
MSE_ridge_df$model <- 'Ridge'

MSE_adalasso_df <- as.data.frame(MSE_adalasso)
names(MSE_adalasso_df) <- c('MSE')
MSE_adalasso_df$year <- format(applied_data_list, '%Y')
MSE_adalasso_df$model <- 'Adalasso'
MSE_har_df <- as.data.frame(MSE_har)
names(MSE_har_df) <- c('MSE')
MSE_har_df$year <- format(applied_data_list, '%Y')
MSE_har_df$model <- 'HAR'

MSE_elastic_net_df <- as.data.frame(MSE_elastic_net)
names(MSE_elastic_net_df) <- c('MSE')
MSE_elastic_net_df$year <- format(applied_data_list, '%Y')
MSE_elastic_net_df$model <- 'Elastic Net'

MSE_ada_elastic_net_df <- as.data.frame(MSE_ada_elastic_net)
names(MSE_ada_elastic_net_df) <- c('MSE')
MSE_ada_elastic_net_df$year <- format(applied_data_list, '%Y')
MSE_ada_elastic_net_df$model <- 'Adaptative Elastic Net'
MSE_df <- bind_rows(MSE_adalasso_df,MSE_ada_elastic_net_df,MSE_har_df, MSE_ridge_df, MSE_lasso_df,
                      MSE_elastic_net_df)

MSE_aggr <- MSE_df %>% group_by(year, model) %>% summarise_at(vars(-group_cols()), mean)
ggplot(MSE_aggr, aes(year, MSE, model)) + geom_point(aes(colour = model)) + geom_path(aes(group = model, colour = model)) + scale_y_continuous(breaks = scales::pretty_breaks(n = 20))
```

```{r}
```


```{r}
bag=boot::tsboot(tseries=tsd,statistic=lynx.fun,R=R,l=l
               ,sim=sim)
```
test
```{r}
original=bag$t0
  bootres=bag$t
  colnames(bootres)=names(original)
```



